<html>
<link rel="stylesheet" type="text/css" href="style.css">
<head>
  <title>Publications related to AI Efficiency</title> 
</head>

<body>

<div class="center">

<h1>Publications Related to AI Efficiency</h1>

<h3>Efficient and Scalable System Strategies for Training Massive Models</h3>

<!-- <a href="https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf">
    (Parameter-Server) Scaling Distributed Machine Learning with the Parameter Server</a><br>
    OSDI 2014
<p> -->

<a href="https://arxiv.org/abs/1909.08053">
    (Tensor-Slicing Parallelism) Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a><br>
    Arxiv 2019
<p>

<a href="https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf">
    (Pipeline Parallelism) GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism<br>
    NeurIPS 2019
<p>

<a href="https://arxiv.org/pdf/1910.02054.pdf">
    (ZeRO-style Data Parallelism) ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a><br>
    SC 2020
<p>


<a href="https://arxiv.org/abs/2104.04473">
    (3D Parallelism) Efficient large-scale language model training on GPU clusters using megatron-LM</a><br>
    SC 2021
<p>    

<a href="https://arxiv.org/pdf/2205.05198">
    (Sequence Parallelism) Reducing Activation Recomputation in Large Transformer Models</a><br>
    Arxiv 2022
<p>

<a href="https://arxiv.org/abs/2310.01889">
    (Sequence Parallelism) Ring Attention with Blockwise Transformers for Near-Infinite Context</a><br>
    Arxiv 2023
<p>

<a href="https://arxiv.org/abs/2401.10241">
    (Pipeline Parallelism) Zero Bubble Pipeline Parallelism</a><br>
    Arxiv 2023
</a>

<!-- <h3>System Optimizations for Training Large Models on Limited GPU Resources</h3> -->

<a href="https://www.usenix.org/system/files/atc21-ren-jie.pdf">
    ZeRO-Offload: Democratizing Billion-Scale Model Training</a><br>
    USENIX ATC 2021
<p>

<a href="https://arxiv.org/abs/2104.07857">
    ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</a><br>
    SC 2021
<p>

<a href="https://arxiv.org/abs/2306.10209">
    ZeRO++: Extremely Efficient Collective Communication for Giant Model Training</a><br>
    NeurIPS 2024
<p> 

<a href="https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin">
    Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning</a><br>
    OSDI 2022
<p> 

<a href="https://arxiv.org/abs/1710.03740">
    Mixed Precision Training</a><br>
    Arxiv 2017
<p> 

<a href="https://arxiv.org/abs/1604.06174">
    (Gradient checkpointing aka rematerialization) Training Deep Nets with Sublinear Memory Cost</a><br>
    Arxiv 2016
<p>

<a href="https://openreview.net/pdf?id=gmmXyAq8TI">
    Coop: Memory is not a Commodity</a><br>
    NeurIPS 2023
<p>  

<a href="https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf">
    Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a><br>
    JMLR 2022  
<p> 

<a></a>

<h3>System Optimizations for Low Inference Latency and Cost</h3>

<a href="https://arxiv.org/abs/2205.14135">
    FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a><br>
    NeurIPS 2022
<p>

<a href="https://arxiv.org/abs/2307.08691">
    FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a><br>
    Arxiv 2023
<p>    

<!-- <a href="https://arxiv.org/pdf/2207.00032.pdf">
    DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale</a><br>
    SC 2022
<p>     -->

<!-- <a href="https://arxiv.org/pdf/2210.03052.pdf">
    ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs</a><br>
    IPDPS 2023 Best Paper
<p>
     -->
<a href="https://arxiv.org/pdf/2211.05102">
    Efficiently Scaling Transformer Inference</a><br>
    MLSys 2023 Best Paper
<p>
    
     

<a href="https://arxiv.org/abs/2309.06180">
    (vLLM) Efficient Memory Management for Large Language Model Serving with PagedAttention</a><br>
    SOSP 2023
<p>

<!-- <a href="https://arxiv.org/pdf/2303.06865.pdf">
    FlexGen: High-throughput Generative Inference of Large Language Models with a Single GPU</a><br>
    ICML 2023
<p> -->

<a href="https://arxiv.org/pdf/2312.07104">
    SGLang: Efficient Execution of Structured Language Model Programs</a><br>
    Arxiv 2024
<p>

<a href="https://arxiv.org/abs/2405.04437">
    vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention</a><br>
    Arxiv 2024
<p>

<a href="https://www.usenix.org/conference/osdi18/presentation/chen">
    TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</a><br>
    OSDI 2018
<p>

<a href="https://dl.acm.org/doi/10.1145/3315508.3329973">
    Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations</a><br>
    MAPL 2019
<p>

<h3>Efficient Algorithms to Make DL Models Smaller, Faster, and Cheaper</h3>

<a href="https://openreview.net/pdf?id=gmmXyAq8TI">
    AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a><br>
    NeurIPS 2023
<p>

<a href="https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf">
    SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a><br>
    ICML 2023
<p>

<a href="https://arxiv.org/abs/2210.17323">
    GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a><br>
    ICLR 2023
<p>

<!-- <a href="https://arxiv.org/abs/2310.17157">
    Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time</a><br>
    ICLR 2023
<p> -->

<a href="https://arxiv.org/abs/2306.14048">
    H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</a><br>
    ICML 2023
<p>

<a href="https://hanlab.mit.edu/projects/streamingllm">
    Efficient Streaming Language Models with Attention Sinks</a><br>
    Arxiv 2023
<p>    

<a href="https://arxiv.org/abs/2211.17192">
    Fast Inference from Transformers via Speculative Decoding</a><br>
    ICML 2023
<p> 

<a href="https://arxiv.org/abs/2305.14314">
    QLoRA: Efficient Finetuning of Quantized LLMs
</a><br>
    NeurIPS 2023
<p> 
    

<!-- <h3>System and Algorithm Co-Design for Efficient Training and Inference</h3> -->

<!-- <a href="https://personal.stevens.edu/~hliu77/docs/sc21a.pdf">
    E.T.: re-thinking self-attention for transformer models on GPUs</a><br>
    SC 2021
<p>  -->
    
<!-- <a href="https://arxiv.org/pdf/2309.17224.pdf">Training and Inference of Large Language Models using 8-bit Floating Point</a><br>
    Arxiv 2023  
</div> -->

<h3>Efficiency Improvements for Emerging Models and Applications</h3>

<a href="https://arxiv.org/abs/2310.04378">Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference</a><br>
    Arxiv 2023  
    <p> 

<a href="hhttps://elanapearl.github.io/blog/2024/the-illustrated-alphafold/">The Illustrated AlphaFold
</a><br>
    Blog 2024  
    <p> 

<!-- <a href="https://arxiv.org/abs/2308.08155">AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation
</a><br>
    Arxiv 2023  
<p>  -->

<a href="https://arxiv.org/abs/2306.00978">
    CAGRA: Highly Parallel Graph Construction and Approximate Nearest Neighbor Search for GPUs</a><br>
    ICDE 2024  
<p> 

<a href="https://arxiv.org/pdf/2104.05158">Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models</a><br>
ISCA 2022  
<p> 

</body>
</html>