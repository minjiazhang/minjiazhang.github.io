<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head><meta charset="utf-8"><meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">
	<link href="./jemdoc.css" rel="stylesheet" type="text/css" />
	<title>2024 Fall CS 598 AIE - AI Efficiency: Systems & Algorithms</title>
</head>
<body>
<table id="tlayout" summary="Table for page layout.">
	<tbody>
		<tr valign="top">
			<td id="layout-menu">
			<!-- <div class="menu-category">main</div> -->

			<div class="menu-item"><a class="current" href="https://minjiazhang.github.io/courses/cs-aie-598-2024fall.html">Course Home</a></div>

			<!-- <div class="menu-category">schedule</div> -->

			<div class="menu-item"><a href="https://minjiazhang.github.io/courses/24fall-schedule.html">Fall 2024</a></div>

			<!-- <div class="menu-category">assignments</div>

			<div class="menu-item"><a href="https://yxw.cs.illinois.edu/course/CS598ACV/S21/assignments.html">overview</a></div> -->
			</td>
			<td id="layout-content">
			<div id="toptitle">
			<h1>(2024 Fall) CS 598 AIE - AI Efficiency: Systems & Algorithms</h1>

			<div id="subtitle"><a name="top"></a></div>
			</div>

			<h2>Schedule (Tentative)</h2>

			<!-- <p>We will typically cover two papers in each class.</p> -->
			<table>
				<tbody>
					<tr class="r1">
						<td class="c1"><strong>Date</strong></td>
						<td class="c2"><strong>Presenter</strong></td>
						<td class="c3"><strong>Topics/Readings</strong></td>
						<!-- <td class="c4">Papers</td> -->
						<td class="c5"><strong>Papers</strong></td>
						<td class="c5"><strong>Slides</strong></td>
						<td class="c6"><strong>Selected Review</strong></td>
					</tr>
					<tr class="r2">
						<td class="c1">Aug 28</td>
						<td class="c2">Minjia Zhang</td>
						<td class="c3">Course Introduction</td>
						<!-- <td class="c4"></td> -->
						<td class="c5"></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r3">
						<td class="c1">Aug 30</td>
						<td class="c2">Minjia Zhang</td>
						<td class="c3">Training Efficiency</td>
						<td class="c4"></td>
						<td class="c5"><a href="fall24-resource/slides/AI-Efficiency-Training.pdf">slides</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r4">
						<td class="c1">Sept 4</td>
						<td class="c2">Minjia Zhang</td>
						<td class="c3">Serving Efficiency</td>
						<td class="c4"></td>
						<td class="c5"><a href="fall24-resource/slides/AI-Efficiency-Serving.pdf">slides</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r5">
						<td class="c1">Sept 6</td>
						<td class="c2">Minjia Zhang</td>
						<td class="c3">DeepSpeed Library</td>
						<!-- <td class="c4"></td> -->
						<td class="c5"></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>

                    <tr>
						<td class="c1"></td>
						<td class="c2" colspan="5"><strong>System Optimizations for Training Massive Models</strong></td>
					</tr>
					<tr class="r5">
						<td class="c1">Sept 11</td>
						<td class="c2">Nicholas Satchanov</td>
						<td class="c3">(Tensor-slicing parallelism) Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism
						</td>
						<td class="c5"><a href="fall24-resource/Megatron-LM.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r6">
						<td class="c1">Sept 11</td>
						<td class="c2">Jiachen Yuan</td>
						<td class="c3">(Pipeline Parallelism) GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
						</td>
						<td class="c5"><a href="fall24-resource/GPipe.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r7">
						<td class="c1">Sept 13</td>
						<td class="c2">Qinjun Jiang, <br /> Tong Wei</td>
						<td class="c3">(3D parallelism) Efficient large-scale language model training on GPU clusters using megatron-LM
						</td>
						<td class="c5"><a href="fall24-resource/3D parallelism.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r8">
						<td class="c1">Sept 13</td>
						<td class="c2">Yihe Zhang</td>
						<td class="c3">(Sequence Parallelism) Reducing Activation Recomputation in Large Transformer Models
						</td>
						<td class="c5"><a href="fall24-resource/Sequence Parallelism.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r9">
						<td class="c1">Sept 18</td>
						<td class="c2">Jiankun Wang</td>
						<td class="c3">(Sequence Parallelism) Ring Attention with Blockwise Transformers for Near-Infinite Context
						</td>
						<td class="c5"><a href="fall24-resource/Ring Attention.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r10">
						<td class="c1">Sept 18</td>
						<td class="c2">Sizheng Zhang</td>
						<td class="c3">(ZeRO-style Data Parallelism) ZeRO: Memory Optimizations Toward Training Trillion Parameter Models
						</td>
						<td class="c5"><a href="fall24-resource/zero.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>

					</tr>
					<tr class="r11">
						<td class="c1">Sept 20</td>
						<td class="c2">Yueming Yuan</td>
						<td class="c3">ZeRO-Offload: Democratizing Billion-Scale Model Training
						</td>
						<td class="c5"><a href="fall24-resource/ZeRO-Offload.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r12">
						<td class="c1">Sept 20</td>
						<td class="c2">Nikhil Kanamarla, <br /> Xinyi Song</td>
						<td class="c3">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning
						</td>
						<td class="c5"><a href="fall24-resource/ZeRO-Infinity.pdf">pdf</a></td>
					</tr>
					<tr class="r13">
						<td class="c1">Sept 25</td>
						<td class="c2">Bhagyashree Taleka, <br /> Rahul Bothra</td>
						<td class="c3">ZeRO++: Extremely Efficient Collective Communication for Giant Model Training
						</td>
						<td class="c5"><a href="fall24-resource/ZeRO++.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r14">
						<td class="c1">Sept 25</td>
						<td class="c2">Zelei Shao</td>
						<td class="c3">Mixed Precision Training
						</td>
						<td class="c5"><a href="fall24-resource/Mixed Precision Training.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r15">
						<td class="c1">Sept 27</td>
						<td class="c2">Khoa Pham, <br /> Julian Yu</td>
						<td class="c3">(Auto-Parallelism) Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning
						</td>
						<td class="c5"><a href="fall24-resource/alpa.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r16">
						<td class="c1">Sept 27</td>
						<td class="c2">Shuning Zhang</td>
						<td class="c3">(Pipeline Parallelism) Zero Bubble Pipeline Parallelism
						</td>
						<td class="c5"><a href="fall24-resource/Pipeline Parallelism.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r17">
						<td class="c1">Oct 2</td>
						<td class="c2">Aryan Bhardwaj</td>
						<td class="c3">(Gradient checkpointing aka rematerialization) Training Deep Nets with Sublinear Memory Cost
						</td>
						<td class="c5"><a href="fall24-resource/Training Deep Nets with Sublinear Memory Cost.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r18">
						<td class="c1">Oct 2</td>
						<td class="c2">Sultan</td>
						<td class="c3">Coop: Memory is not a Commodity
						</td>
						<td class="c5"><a href="fall24-resource/Coop- Memory is not a Commodity .pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>

                    <tr>
						<td class="c1"></td>
						<td class="c2" colspan="5"><strong>System Optimizations for Low Inference Latency and Cost</strong></td>
					</tr>
					<tr class="r19">
						<td class="c1">Oct 4</td>
						<td class="c2">Nachuan Wang</td>
						<td class="c3">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
						</td>
						<td class="c5"><a href="fall24-resource/FlashAttention-2- Faster Attention with Better Parallelism and Work Partitioning .pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r20">
						<td class="c1">Oct 4</td>
						<td class="c2">Hanyang Chen, <br /> Divya Koya</td>
						<td class="c3">Efficiently Scaling Transformer Inference
						</td>
						<td class="c5"><a href="fall24-resource/Efficiently Scaling Transformer Inference.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r21">
						<td class="c1">Oct 9</td>
						<td class="c2">Neel Dani, <br /> Jianping Li</td>
						<td class="c3">(vLLM) Efficient Memory Management for Large Language Model Serving with PagedAttention
						</td>
						<td class="c5"><a href="fall24-resource/vllm.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r22">
						<td class="c1">Oct 9</td>
						<td class="c2">Amaan Khan, <br /> Akul Gupta</td>
						<td class="c3">vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention
						</td>
						<td class="c5"><a href="fall24-resource/vAttention.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r23">
						<td class="c1">Oct 11</td>
						<td class="c3" colspan="3"><strong>No Class: Prepare for Proposal</strong>
						</td>
					</tr>
					<tr>
						<td class="c1">Oct 13</td>
						<td class="c3" colspan="3"><strong>Proposal Due</strong></td>
					</tr>
					<tr class="r24">
						<td class="c1">Oct 16</td>
						<td class="c2">Sarthak Chakraborty</td>
						<td class="c3">Orca: A Distributed Serving System for Transformer-Based
						</td>
						<td class="c5"><a href="fall24-resource/Orca.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r25">
						<td class="c1">Oct 18</td>
						<td class="c2">Ryan Ziegler, <br /> Yinan Zhao</td>
						<td class="c3">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning
						</td>
						<td class="c5"><a href="fall24-resource/TVM.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r26">
						<td class="c1">Oct 23</td>
						<td class="c2">Krut Patel, <br /> Anay Bhakat</td>
						<td class="c3">Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations
						</td>
						<td class="c5"><a href="fall24-resource/Triton.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr>
						<td class="c1"></td>
						<td class="c2" colspan="5"><strong>Efficient Algorithms to Make DL Models Smaller, Faster, and Cheaper</strong></td>
					</tr>

					<tr class="r27">
						<td class="c1">Oct 25</td>
						<td class="c2">Raunak Shah</td>
						<td class="c3">AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration
						</td>
						<td class="c5"><a href="fall24-resource/awq.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r28">
						<td class="c1">Oct 25</td>
						<td class="c2">Aditi Tiwari</td>
						<td class="c3">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models
						</td>
						<td class="c5"><a href="fall24-resource/SmoothQuant.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r29">
						<td class="c1">Oct 30</td>
						<td class="c2">Muyan Hu</td>
						<td class="c3">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers
							</td>
						<td class="c5"><a href="fall24-resource/GPTQ.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r30">
						<td class="c1">Oct 30</td>
						<td class="c2">Ya-Ting Pai</td>
						<td class="c3">Efficient Streaming Language Models with Attention Sinks
						</td>
						<td class="c5"><a href="fall24-resource/EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r31">
						<td class="c1">Nov 1</td>
						<td class="c2">Xiaocong Yang</td>
						<td class="c3">Fast Inference from Transformers via Speculative Decoding
						</td>
						<td class="c5"><a href="fall24-resource/Fast Inference from Transformers via Speculative Decoding .pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r32">
						<td class="c1">Nov 1</td>
						<td class="c2">Saksham Gera</td>
						<td class="c3">QLoRA: Efficient Finetuning of Quantized LLMs
						</td>
						<td class="c5"><a href="fall24-resource/QLoRA- Efficient Finetuning of Quantized LLMs.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>

					<tr>
						<td class="c1"></td>
						<td class="c2" colspan="5"><strong>Efficiency Improvements for Emerging Real-World Models and Applications</strong></td>
					</tr>
					<tr class="r33">
						<td class="c1">Nov 8</td>
						<td class="c2">Ruize Gao</td>
						<td class="c3">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
						</td>
						<td class="c5"><a href="fall24-resource/Switch Transformers.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r34">
						<td class="c1">Nov 13</td>
						<td class="c2">Jiatong Li</td>
						<td class="c3">CAGRA: Highly Parallel Graph Construction and Approximate Nearest Neighbor Search for GPUs
						</td>
						<td class="c5"><a href="fall24-resource/CAGRA.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r35">
						<td class="c1">Nov 15</td>
						<td class="c2">Ananya Sehgal, <br /> Xin Xu</td>
						<td class="c3">Mamba: Linear-Time Sequence Modeling with Selective State Spaces
						</td>
						<td class="c5"><a href="fall24-resource/Mamba.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r36">
						<td class="c1">Nov 20</td>
						<td class="c2">Aydan Pirani</td>
						<td class="c3">Scalable Diffusion Models with Transformers
						</td>
						<td class="c5"><a href="fall24-resource/Scalable Diffusion Models with Transformers.pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r37">
						<td class="c1">Nov 20</td>
						<td class="c2">Chenghao Mo</td>
						<td class="c3">The Illustrated AlphaFold
						</td>
						<td class="c5"><a href="https://elanapearl.github.io/blog/2024/the-illustrated-alphafold/">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r38">
						<td class="c1">Nov 22</td>
						<td class="c2">Jiaqi Lou, <br /> Haoran Yuan</td>
						<td class="c3">Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models
						</td>
						<td class="c5"><a href="fall24-resource/Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models .pdf">pdf</a></td>
						<td class="c6"></td> <td class="c7"></td>
					</tr>
					<tr class="r39">
						<td class="c1">Nov 27</td>
						<td class="c3" colspan="5"><strong>Fall break</strong>
						</td>
					</tr>
					<tr class="r40">
						<td class="c1">Nov 29</td>
						<td class="c3" colspan="5"><strong>Fall break</strong>
						</td>
					</tr>
					<tr class="r41">
						<td class="c1">Dec 4</td>
						<td class="c3" colspan="5"><strong>Final Presentation</strong>
						</td>
					</tr>
					<tr class="r42">
						<td class="c1">Dec 6</td>
						<td class="c3" colspan="5"><strong>Final Presentation</strong>
						</td>
					</tr>
					<tr class="r43">
						<td class="c1">Dec 13</td>
						<td class="c3" colspan="5"><strong>Final Report Due</strong>
						</td>
					</tr>
				</tbody>
			</table>

			<!-- <div id="footer">
			<div id="footer-text">Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.</div>
			</div> -->
			</td>
		</tr>
	</tbody>
</table>
</body>
</html>
