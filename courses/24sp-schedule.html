
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head><meta charset="utf-8"><meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">
	<link href="./jemdoc.css" rel="stylesheet" type="text/css" />
	<title>CS 598 AIE - AI Efficiency: Systems & Algorithms</title>
</head>
<body>
<table id="tlayout" summary="Table for page layout.">
	<tbody>
		<tr valign="top">
			<td id="layout-menu">
			<!-- <div class="menu-category">main</div> -->

			<div class="menu-item"><a class="current" href="https://minjiazhang.github.io/courses/cs-aie-598.html">Course Home</a></div>

			<!-- <div class="menu-category">schedule</div> -->

			<div class="menu-item"><a href="https://minjiazhang.github.io/courses/24sp-schedule.html">Spring 2024</a></div>

			<!-- <div class="menu-category">assignments</div>

			<div class="menu-item"><a href="https://yxw.cs.illinois.edu/course/CS598ACV/S21/assignments.html">overview</a></div> -->
			</td>
			<td id="layout-content">
			<div id="toptitle">
			<h1>(SP24) CS 598 AIE - AI Efficiency: Systems & Algorithms</h1>

			<div id="subtitle"><a name="top"></a></div>
			</div>

			<h2>Schedule (Tentative)</h2>

			<!-- <p>We will typically cover two papers in each class.</p> -->

			<table>
				<tbody>
					<tr class="r1">
						<td class="c1"><strong>Date</strong></td>
						<td class="c2"><strong>Presenter</strong></td>
						<td class="c3"><strong>Topics/Readings</strong></td>
						<!-- <td class="c4">Papers</td> -->
						<td class="c5"><strong>Slides</strong></td>
					</tr>
					<tr class="r2">
						<td class="c1">Jan 16</td>
						<td class="c2">Minjia Zhang</td>
						<td class="c3">Course Introduction</td>
						<!-- <td class="c4"></td> -->
						<td class="c5"></td>
					</tr>
                    <tr class="r3">
						<td class="c1">Jan 18</td>
						<td class="c2">Minjia Zhang</td>
						<td class="c3">Training Efficiency</td>
						<!-- <td class="c4"></td> -->
						<td class="c5"><a href="sp24-resource/AI-Efficiency-Training.pdf">pdf</a></td>
					</tr>
                    <tr class="r4">
						<td class="c1">Jan 23</td>
						<td class="c2">Minjia Zhang</td>
						<td class="c3">Inference Effiiency</td>
						<!-- <td class="c4"></td> -->
						<td class="c5"><a href="sp24-resource/AI-Efficiency-Algorithms.pdf">pdf</a></td>
					</tr>
                    <tr>
						<td class="c1" colspan="4"><strong>System Optimizations for Training Massive Models</strong></td>
					</tr>
                    <tr class="r5">
						<td class="c1">Jan 25</td>
						<td class="c2">Olatunji Ruwase (Invited speaker)</td>
						<td class="c3">DeepSpeed Library</td>
						<td class="c5"><a href="sp24-resource/DeepSpeed-UIUC-AI_Efficiency-January-2024.pdf">pdf</a></td>
					</tr>
					<tr class="r6">
						<td class="c1">Jan 30</td>
						<td class="c2">Yiqi Liu</td>
						<td class="c3">(ZeRO-style data parallelism) <a href="https://arxiv.org/pdf/1910.02054.pdf">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a> 
                        (SC 2020)<br /> 
                        </td>
						<td class="c5"><a href="sp24-resource/ZeRO.pdf">pdf</a></td>
					</tr>
					<tr class="r7">
						<td class="c1">Feb 1</td>
						<td class="c2">Haoyang Zhang</td>
						<td class="c3"> <a href="https://www.usenix.org/system/files/atc21-ren-jie.pdf">ZeRO-Offload: Democratizing Billion-Scale Model Training</a> (ATC 2021) <br />
						<!-- <br /> -->
						<!-- S. Bansal, V. Tolani, S. Gupta, J. Malik, and C. Tomlin. <a href="http://proceedings.mlr.press/v100/bansal20a/bansal20a.pdf">Combining optimal control and learning for visual navigation in novel environments</a>. CoRL, 2019.-->
                        </td> 
						<td class="c5"><a href="sp24-resource/ZeRO-Offload-pre.pdf">pdf</a></td>
					</tr>
					<tr class="r8">
						<td class="c1">Feb 6</td>
						<td class="c2">Yufeng Du</td>
						<td class="c3">(Tensor-slicing parallelism) <a href="https://arxiv.org/abs/1909.08053">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a> (Arxiv 2019) <br />
						</td>
						<td class="c5"><a href="sp24-resource/Megatron-LM.pdf">pdf</a></td>
					</tr>
					<tr class="r9">
						<td class="c1">Feb 8</td>
						<td class="c2">Siyuan Chai</td>
						<td class="c3"><a href="https://arxiv.org/abs/2104.07857">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</a>
                        (SC 2021)<br />
						<td class="c5"><a href="sp24-resource/SC21-ZeRO-Infinity-pre.pdf">pdf</td>
					</tr>
					<tr class="r10">
						<td class="c1">Feb 13</td>
						<td class="c2">Gangmuk Lim, <br /> Ahan Gupta</td>
						<td class="c3"><a href="https://proceedings.mlr.press/v162/patil22b/patil22b.pdf">POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging</a> (ICML 2022) <br />
						(Sequence parallelism) <a href="https://arxiv.org/pdf/2205.05198.pdf">Reducing Activation Recomputation in Large Transformer Models</a> (Arxiv 2022) <br />
                        </td>
						<td class="c5"><a href="sp24-resource/poet-pre.pdf">pdf</a>  <br /> <a href="sp24-resource/Sequence-parallel-pre.pdf">pdf</a> </td>
					</tr>
                    <!-- <tr class="r11">
						<td class="c1">Feb 13</td>
						<td class="c2">Ahan Gupta</td>
						<td class="c3">(Sequence parallelism) <a href="https://arxiv.org/pdf/2205.05198.pdf">Reducing Activation Recomputation in Large Transformer Models</a> (Arxiv 2022) <br />
                        </td>
						<td class="c5"></td>
					</tr> -->
                    <tr>
						<td class="c1" colspan="4"><strong>System Optimizations for Low Inference Latency and Cost</strong></td>
					</tr>
					<tr class="r12">
						<td class="c1">Feb 15</td>
						<td class="c2">Yuhao Ge, <br /> Yuqi Xue</td>
						<td class="c3"><a href="https://arxiv.org/abs/2307.08691">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a> (NeurIPS 2022) <br />
                        (vLLM) <a href="https://arxiv.org/abs/2309.06180">Efficient Memory Management for Large Language Model Serving with PagedAttention</a> (SOSP 2023) <br />
						</td>
						<td class="c5"><a href="sp24-resource/flash-attention-v2-pre.pdf">pdf</a> <br /> <a href="sp24-resource/vLLM-pre.pdf">pdf</td>
					</tr>
                    <tr class="r13">
						<td class="c1">Feb 20</td>
						<td class="c2">Yanzhuo Chen</td>
						<td class="c3"><a href="https://arxiv.org/pdf/2210.03052.pdf">ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs</a> (IPDPS 2023 Best Paper) <br />
                        </td>
						<td class="c5"><a href="sp24-resource/ByteTransformer-pre.pdf">pdf</td>
					</tr>
					<tr class="r13b">
						<td class="c1">Feb 22</td>
						<td class="c2">Aditya Prerepa</td>
						<td class="c3">
                        <a href="https://www.usenix.org/conference/osdi22/presentation/yu">Orca: A Distributed Serving System for Transformer-Based Generative Models
                        </a> (OSDI 2022) <br />
						</td>
						<td class="c5"><a href="sp24-resource/ORCA-pre.pdf">pdf</td>
					</tr>
                    <!-- <tr class="r14">
						<td class="c1">Feb 27</td>
						<td class="c2">Minyang Tian</td>
						<td class="c3"><a href="https://arxiv.org/pdf/2207.00032.pdf">DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale</a> (SC 2022) <br />
						</td>
						<td class="c5"></td>
					</tr> -->
					<tr class="r14">
                        <td class="c1">Feb 27</td>
                        <td class="c2">Vignesh Suresh</td>
                        <td class="c3"><a href="https://arxiv.org/pdf/2211.05102.pdf">Efficiently Scaling Transformer Inference</a> (Arxiv 2022) <br />                         
                        </td>
                        <td class="c5"></td>
                    </tr>
                    <tr class="r15">
                        <td class="c1">Feb 29</td>
                        <td class="c2">Steven Gao</td>
                        <td class="c3">
                        <a href="https://arxiv.org/pdf/2303.06865.pdf">FlexGen: High-throughput Generative Inference of Large Language Models with a Single GPU</a> (ICML 2023) <br />
                        </td>
                        <td class="c5"></td>
                    </tr>
                    <tr>
						<td class="c1" colspan="4"><strong>Efficient Algorithms to Make DL Models Smaller, Faster, and Cheaper</strong></td>
					</tr>
                    <tr class="r17">
                        <td class="c1">Mar 5</td>
                        <td class="c2">Xinyu Lian, <br /> Mayank Bhatia</td>
                        <td class="c3"><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/adf7fa39d65e2983d724ff7da57f00ac-Paper-Conference.pdf">ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</a> (NeurIPS 2022) <br /> 
                        <a href="https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a> (ICML 2023) <br />
                        </td>
                        <td class="c5"></td>
                    </tr>
					<tr>
						<td class="c1" colspan="4"><strong>Pre-proposal meetings</strong></td>
					</tr>
                    <tr class="r16">
                        <td class="c1">Mar 7</td>
                        <td class="c2">Students & Instructors</td>
                        <td class="c3">Pre-proposal: 15 minutes each group (need schedule ahead-of-time)</td>
                        <td class="c5"></td>
                    </tr>
					<tr>
						<td class="c1" colspan="4"><strong>Efficient Algorithms to Make DL Models Smaller, Faster, and Cheaper</strong></td>
					</tr>
                    <tr class="r18">
                        <td class="c1">Mar 19</td>
                        <td class="c2">Selin Yildirim, <br /> Tian Luan</td>
                        <td class="c3"><a href="https://arxiv.org/abs/2310.17157">Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time</a> (ICLR 2023) <br /> 
                        <a href="https://arxiv.org/abs/2306.14048">H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</a> (ICML 2023) <br />
                        </td>
                        <td class="c5"></td>
                    </tr>
                    <tr class="r19">
                        <td class="c1">Mar 21</td>
                        <td class="c2">Akhil Bhimaraju, <br /> Lingzhi Zhao</td>
                        <td class="c3"><a href="https://hanlab.mit.edu/projects/streamingllm">Efficient Streaming Language Models with Attention Sinks</a> (Arxiv 2023) <br /> 
                        <a href="https://arxiv.org/abs/2211.17192">Fast Inference from Transformers via Speculative Decoding</a> (ICML 2023) <br />
                        </td>
                        <td class="c5"></td>
                    </tr>
                    <tr class="r20">
                        <td class="c1">Mar 26</td>
                        <td class="c2">Akshat Sharma, <br /> Henry Zhu</td>
                        <td class="c3">
						<a href="https://arxiv.org/abs/1710.03740">Mixed Precision Training</a> (Arxiv 2017) <br /> 
						<a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a> (NeurIPS 2023) <br /> 
                        </td>
                        <td class="c5"></td>
                    </tr>
                    <tr>
						<td class="c1" colspan="4"><strong>System and Algorithm Co-Design for Efficient Training and Inference</strong></td>
					</tr>
                    <tr class="r21">
                        <td class="c1">Mar 28</td>
                        <td class="c2">Wanyu Zhao</td>
                        <td class="c3">
                        <a href="https://personal.stevens.edu/~hliu77/docs/sc21a.pdf">E.T.: re-thinking self-attention for transformer models on GPUs</a> (SC 2021) <br />
                        </td>
                        <td class="c5"></td>
                    </tr>
                    <tr class="r22">
                        <td class="c1">Apr 2</td>
                        <td class="c2">Bakshree Mishra</td>
                        <td class="c3"><a href="https://arxiv.org/pdf/2309.17224.pdf">Training and Inference of Large Language Models using 8-bit Floating Point</a> (Arxiv 2023) <br />
                        </td>
                        <td class="c5"></td>
                    </tr>
                    <tr class="r23">
						<td class="c1">Apr 9</td>
						<td class="c2">Wei Wen (Invited speaker)</td>
						<td class="c3">Invited Talk on NAS + DLRM from Meta</td>
						<td class="c5"></td>
					</tr>
                    <tr class="r24">
						<td class="c1">Apr 11</td>
						<td class="c2">Chunyuan Li (Invited speaker)</td>
						<td class="c3">Invited Talk on multi-modal</td>
						<td class="c5"></td>
					</tr>
                    <tr>
						<td class="c1" colspan="4"><strong>Efficiency Improvements for Emerging Real-World Models and Applications</strong></td>
					</tr>
                    <tr class="r25">
                        <td class="c1">Apr 4</td>
                        <td class="c2">Ritik Dutta</td>
                        <td class="c3"><a href="https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a> (JMLR 2022) <br />
                        </td>
                        <td class="c5"></td>
                    </tr>
                    <tr class="r26">
                        <td class="c1">Apr 16</td>
                        <td class="c2">James Soole</td>
                        <td class="c3"><a href="https://arxiv.org/abs/2309.06380">InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation</a> (Arxiv 2023) <br />
                        </td>
                        <td class="c5"></td>
                    </tr>
                    <tr class="r27">
                        <td class="c1">Apr 18</td>
                        <td class="c2">Hari Umesh</td>
                        <td class="c3"><a href="https://arxiv.org/abs/2312.12458">When Parameter-efficient Tuning Meets General-purpose Vision-language Models</a> (Arxiv 2023) <br />
                        </td>
                        <td class="c5"></td>
                    </tr>
                    <tr class="r28">
                        <td class="c1">Apr 23</td>
                        <td class="c2">Tanay Dixit</td>
                        <td class="c3"><a href="https://arxiv.org/abs/2308.08155">AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</a> (Arxiv 2023) <br />
                        </td>
                        <td class="c5"></td>
                    </tr>
                    <tr class="r29">
                        <td class="c1">Apr 25</td>
                        <td class="c2">Haocheng Shen</td>
						<td class="c3"><a href="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a> (Arxiv 2023) <br />
                        </td>
                        <td class="c5"></td>
                    </tr>
                    <tr class="r30">
                        <td class="c1">Apr 30</td>
                        <td class="c2">Zhenrui Yue</td>
                        <td class="c3"><a href="https://arxiv.org/abs/2112.04426">Improving Language Models by Retrieving from Trillions of Tokens</a> (ICML 2022) <br />
                        </td>
                        <td class="c5"></td>
                    </tr>
					<tr class="r31">
						<td class="c1">TBD</td>
						<td class="c2"></td>
						<td class="c3">Final Project Presentations</td>
						<!-- <td class="c4">8:00 AM - 11:00 AM (CT)</td> -->
						<td class="c5"></td>
					</tr>
				</tbody>
			</table>

			<!-- <div id="footer">
			<div id="footer-text">Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.</div>
			</div> -->
			</td>
		</tr>
	</tbody>
</table>
</body>
</html>