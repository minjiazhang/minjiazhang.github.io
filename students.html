<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html>

<hr style="color:rgb(218, 218, 218);">

Over the years, I have been fortunate to work with many talented and hard-working students. 

<h1><a name="students"; style="color:#40A6FF;">Students</a></h1>
<a></a> 
    <h2>Past Interns and Advised Students</h2>
    <ul>
        <li> Chengming Zhang (co-advised with Leon Song), summer Ph.D. intern from Indiana University Bloomington, April--September 2023
            <ul>                
                <li><a href="https://arxiv.org/abs/2309.14509">DeepSpeed-Ulysses</a>, <a href="https://arxiv.org/abs/2310.04610">DeepSpeed4Science</a></li>
            </ul>
        <li> Xinyue Ma (co-advised with Myeongjae Jeon from Ulsan National Institute of Science and Technology), April 2022-June 2023 </li>
            <ul>
                <li><a href="https://arxiv.org/abs/2308.06053">Cost-effective On-device Continual Learning over Memory Hierarchy with Miro</a>, published at MobiCom 2023</li>
            </ul>
        <li> Jiangfei Duan and Ziang Song (co-advised with Zhihao Jia from CMU), June 2022-December 2022</li>
            <ul>
                <li><a href="https://hsword.github.io/assets/pdf/nsdi2024-parcae.pdf">Parcae: Proactive, Liveput-Optimized DNN Training on Preemptible Instances</a>, published at NSDI 2024</li>
            </ul>
        <li> Yongye Su (co-advised with Jianguo Wang from Purdue University), July 2022-September 2023</li>
            <ul>
                <li>Serverless vector search database, published at SIGMOD 2024</li>
            </ul>
        <li> Shuangyan Yang (co-advised with Dong Li from UC Merced), 2021 June--Present</li>
            <ul>
                <li><a href="https://dl.acm.org/doi/10.1145/3575693.3575725">Betty: Enabling Large-Scale GNN Training with Batch-Level Graph Partitioning</a>, published at ASPLOS 2023</li>
            </ul>
        <li> Yucheng Lu, summer Ph.D. intern from Cornell University, June 2021-Feburary 2022</li>
            <ul>
                <li><a href="https://arxiv.org/abs/2202.06009">Maximizing the communication efficiency of DNN training with 0/1 Adam</a>, published at ICML 2023</li>
            </ul>
        <li> Soobee Lee (co-advised with Myeongjae Jeon from Ulsan National Institute of Science and Technology), September 2020-November 2021</li>
            <ul>
                <li><a href="https://dl.acm.org/doi/10.1145/3489517.3530587">CarM: hierarchical episodic memory for continual learning</a>, published at DAC 2022</li>
            </ul>
        <li> Connor Holmes (co-advised with Bo Wu from Colorado School of Mines), June 2020--September 2022</li>
            <ul>
                <li><a href="https://arxiv.org/abs/2110.15766">NxMTransformer: Semi-Structured Sparsification for Natural Language Understanding via ADMM</a>, published at NeurIPS 2021</li>
                <li><a href="https://arxiv.org/abs/2206.15014"></a>Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding</li>
            </ul>
        <li> Hongyi Wang, summer Ph.D. intern from University of Wisconsin Madison, June 2020-September 2020</li>
            <ul>
                <li><a>Efficient DNN training via elastic tranining and low-rank decomposition</a></li>
            </ul>
        <li> Zhen Peng (co-advised with Bin Ren from College of Walliam and Mary and Ruoming Jin from Kent University), Sep 2019-June 2021</li>
            <ul>
                <li><a href="https://dl.acm.org/doi/pdf/10.1145/3572848.3577527">iQAN: Fast and Accurate Vector Search with Efficient
                    Intra-Query Parallelism on Multi-Core Architectures</a>, published at PPoPP 2023</li>
            </ul>
        <li> Jie Ren (co-advised with Dong Li from University of California at Merced), June 2019-December 2020</li>
            <ul>
                <li><a href="https://ieeexplore.ieee.org/abstract/document/9407112">Sentinel: Efficient tensor migration and allocation on heterogeneous memory systems for deep learning</a>, published at HPCA 20213</li>
                <li><a href="https://proceedings.neurips.cc/paper/2020/file/788d986905533aba051261497ecffcbb-Paper.pdf">Hm-ann: Efficient billion-point nearest neighbor search on heterogeneous memory</a>, published at NeurIPS 2020</li>
                <li><a href="https://www.usenix.org/system/files/atc21-ren-jie.pdf">ZeRO-Offload: Democratizing Billion-Scale Model Training</a>, published at USENIX ATC 2021</li>
            </ul>
        <li> Dantong Zhu, visiting scholar, Janurary 2020-June 2020</li>
            <ul>
                <li><a href="https://arxiv.org/abs/2107.13052">Understanding and Generalizing Monotonic Proximity Graphs for Approximate Nearest Neighbor Search</a></li>
            </ul>
        <li> Zehua Hu, intern student from Peking University, July 2019-March 2021</li>
            <ul>
                <li><a href="https://ieeexplore.ieee.org/document/9460468">DUET: A Compiler-Runtime Subgraph Scheduling Approach for Tensor Programs on a Coupled CPU-GPU Architecture</a>, published at IPDPS 2021</li>
            </ul>
        <li> Menghao Li, intern student from Peking University, February 2020-March 2021</li>
            <ul>
                <li><a href="https://proceedings.neurips.cc/paper/2020/hash/a9b7ba70783b617e9998dc4dd82eb3c5-Abstract.html">AdaTune: Adaptive Tensor Program Compilation Made Efficient</a>, published at NeurIPS 2020</li>
                <li><a href="https://openreview.net/pdf?id=GTGb3M_KcUl">DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation</a>, published at NeurIPS 2021</li>
            </ul>
        <li> Conglong Li (co-advised with Dave Andersen from CMU), May 2019-August 2019 </li>
            <ul>
                <li><a href="https://dl.acm.org/doi/10.1145/3318464.3380600">Improving Approximate Nearest Neighbor Search through Learned Adaptive Early Termination</a>, published at SIGMOD 2020</li>
            </ul>
        <li> Stephen Zhou, summer PhD intern from MIT, June 2018-August 2018 </li>
            <ul>
                <li>Automatic model optimization via deep learning compiler</li>
            </ul>
    </ul>

</html>