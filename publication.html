<p></p>
<h1><a name="publications"; style="color:#40A6FF;">Publications</h1>
<a></a> 

        
<h2>Preprint</h2>
<ul>
    <li><b>Preprint</b>                     "<em>RenAIssance: A Survey into AI Text-to-Image Generation in the Era of Large Model</em>", Fengxiang Bie, Yibo Yang, Zhongzhu Zhou, Adam Ghanem, <b>Minjia Zhang</b>, Zhewei Yao, Xiaoxia Wu, Connor Holmes, Pareesa Golnari, David A. Clifton, Yuxiong He, Dacheng Tao, Shuaiwen Leon Song
    <li><b>Preprint</b>                     "<em>DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales</em>", Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, <b>Minjia Zhang</b>, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, Yuxiong He
    <li><b>Preprint</b>                   "<em>DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention</em>", Zhewei Yao, Xiaoxia Wu, Conglong Li, <b>Minjia Zhang</b>, Heyang Qi, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He
    <li><b>Preprint</b>   "<em>Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding</em>",  Connor Holmes, <b>Minjia Zhang</b>, Yuxiong He, Bo Wu
</ul>     
  

<h2>2024</h2>
<ul>
      <li> <b>NSDI 2024</b>  "<em>Parcae: Proactive, Liveput-Optimized DNN Training on Preemptible Instances</em>", Jiangfei Duan, Ziang Song, Xupeng Miao, Xiaoli Xi, Dahua Lin, Harry Xu, <b>Minjia Zhang</b>, Zhihao Jia</li>
      <li> <b>AAAI 2024</b> "<em>DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing</em>", Conglong Li, Zhewei Yao, Xiaoxia Wu, <b>Minjia Zhang</b>, Connor Holmes, Cheng Li, Yuxiong He</li>
      <li><b>ICLR 2024 (Oral)</b> "<em>Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs</em>", Suyu Ge, Yunan Zhang, Liyuan Liu, <b>Minjia Zhang</b>, Jiawei Han, Jianfeng Gao</li>
      <li><b>SIGMOD 2024</b> "<em>Vexless: A Serverless Vector Data Management System Using Cloud Functions</em>", Yongye Su, Yinqi Sun, <b>Minjia Zhag</b>, Jianguo Wang </li>
      <li><b>Nature Methods 2024</b>                  "<em>OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization</em>", with Gustaf Ahdritz, Nazim Bouatta, et. al.</li>
</ul>

<h2>2023</h2>
<ul>    
    <li> <b>ICLR  2023</b> "<em>Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam</em>", Yucheng Lu, Conglong Li, <b>Minjia Zhang</b>, Christopher De Sa, Yuxiong He</li>
    <li> <b>ASPLOS 2023</b>  "<em>Betty: Enabling Large-Scale GNN Training with Batch-Level Graph Partitioning</em>", Shuangyan Yang, <b>Minjia Zhang</b>, Wenqian Dong, Dong Li</li>
    <li> <b>NSDI 2023</b> "<em>Bamboo: Making Preemptible Instances Resilient for Affordable Training of Large DNNs</em>", John Thorpe, Pengzhan Zhao, Jonathan Eyolfson, Yifan Qiao, Zhihao Jia, <b>Minjia Zhang</b>, Ravi Netravali, Guoqing Harry Xu</li>
    <li> <b>MobiCom 2023</b>                 "<em>Cost-effective On-device Continual Learning over Memory Hierarchy with Miro</em>", Xinyue Ma, Suyeon Jeong, <b>Minjia Zhang</b>, Di Wang, Jonghyun Choi, Myeonjae Jeon
    <li> <b>PPoPP 2023</b>                      "<em>iQAN: Fast and Accurate Vector Search with Efficient Intra-Query Parallelism on Multi-Core Architectures</em>", Zhen Peng, <b>Minjia Zhang</b>, Kai Li, Ruoming Jin, Bin Ren
    <li> <b>ECAI 2023</b>  "<em>Revisiting the Efficiency-Accuracy Tradeoff in Adapting Transformer Models via Adversarial Fine-Tuning</em>", <b>Minjia Zhang</b>, Niranjan Uma Naresh, Yuxiong He</li>
    <li> <b>IEEE Data Engineering Bulletin 2023</b> "<em>Exploiting Modern Hardware Architectures for High-Dimensional Vector Search at Speed and Scale</em>", <b>Minjia Zhang</b>, Jie Ren, Zhen Peng, Ruoming Jin, Dong Li, and Bin Ren</li>      
    <li> <b>NeurIPS 2023 AI4Science Workshop</b>    "<em>DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies</em>",  Shuaiwen Leon Song, Bonnie Kruft, <b>Minjia Zhang</b>, Conglong Li, Shiyang Chen, Chengming Zhang, and all
</ul>

<h2>2022</h2>
<ul>
    <li><b>ICML 2022</b>  "<em>Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale</em>", Samyam Rajbhandari, Conglong Li, Zhewei Yao, <b>Minjia Zhang</b>, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He</li>
    <li><b>ACL 2022 Workshop BigScience</b>   "<em>BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</em>" </li>
    <li><b>WSDM 2022</b>                    "<em>GraSP: Optimizing Graph-based Nearest Neighbor Search with Subgraph Sampling and Pruning</em>", <b>Minjia Zhang</b>, Wenhan Wang, Yuxiong He
    <li><b>DAC 2022</b>                           "<em>CarM: Hierarchical Episodic Memory for Continual Learning</em>", Soobee Lee, Minindu Weerakoon, Jonghyun Choi, <b>Minjia Zhang</b>, Di Wang, Myeongjae Jeon
    <li><b>SC 2022</b>  "<em>Enabling Efficient Inference of Transformer Models at Unprecedented Scale</em>", Reza Yazdani Aminabadi,  Samyam Rajbhandari, <b>Minjia Zhang</b>, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Yuxiong He</li> 
    <li><b>MLSys Workshop 2022</b>  "<em>A Survey of Multi-Tenant Deep Learning Inference on GPU</em>", Fuxun Yu, Di Wang, Longfei Shangguan, <b>Minjia Zhang</b>, Chenchen Liu, Xiang Chen, in the MLSYS'22 workshop on Cloud Intelligence/AIOps</li>
    <li><b>TECS 2022</b>  "<em>SHARP: An Adaptable, Energy-Efficient Accelerator for Recurrent Neural Network</em>", Reza Yazdani, Olatunji Ruwase, <b>Minjia Zhang</b>, Yuxiong He, Jose-Maria Arnau, Antonio Gonzalez, in the ACM Transactions on Embedded Computing Systems 2022</li>    
    <li><b>NeurIPS 2022 (Spotlight)</b> "<em>The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models</em>", Conglong Li, <b>Minjia Zhang</b>, Yuxiong He</li>
    <li><b>NeurIPS 2022 (Oral)</b>  "<em>Extreme Compression for Pre-trained Transformers Made Simple and Efficient</em>", Xiaoxia Wu, Zhewei Yao, <b>Minjia Zhang</b>, Conglong Li, Yuxiong He
    <li><b>NeurIPS 2022 (Spotlight)</b>   "<em>ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</em>", Zhewei Yao, Reza Yazdani Aminabadi, <b>Minjia Zhang</b>, Xiaoxia Wu, Conglong Li, Yuxiong He
    <li><b>AAAI 2022</b>   "<em>Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-Trained Transformers</em>", <b>Minjia Zhang</b>, Niranjan Uma Naresh, Yuxiong He
    
</ul>

<h2>2021</h2>
<ul>
    <li><b>USENIX ATC 2021</b>  "<em>ZeRO-Offload: Democratizing Billion-Scale Model Training</em>", Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, <b>Minjia Zhang</b>, Dong Li, Yuxiong He</li>
    <li><b>HPCA 2021</b>  "<em>Efficient Tensor Migration and Allocation on Heterogeneous Memory Systems for Deep Learning</em>", Jie Ren, Jiaolin Luo, Kai Wu, <b>Minjia Zhang</b>, Hyeran Jeon, Dong Li</li>
    <li><b>NeurIPS 2021</b>  "<em>NxMTransformer: Semi-Structured Sparsification for Natural Language Understanding via ADMM</em>", Connor Holmes, <b>Minjia Zhang</b>, Yuxiong He, Bo Wu
    <li><b>ICLR 2021</b>                       "<em>DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation</em>", <b>Minjia Zhang</b>*, Menghao Li*, Chi Wang, Minqin Li. 
    <li><b>IPDPS 2021</b>                     "<em>DUET: Compiler-Aware Subgraph Scheduling for Tensor Programs on a Coupled CPU-GPU Architecture</em>", <b>Minjia Zhang</b>*, Zehua Hu*, Minqin Li. *Equal contribution.
</ul>

<h2>2020</h2>
<ul>
    <li><b>NeurIPS 2020</b>                "<em>AdaTune: Adaptive Tensor Program Compilation Made Efficient</em>", Menghao Li*, <b>Minjia Zhang</b>*, Chi Wang, Minqin Li. *Equal contribution.
    <li><b>NeurIPS 2020</b>  "<em>Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping</em>", <b>Minjia Zhang</b>, Yuxiong He</li>
    <li><b>NeurIPS 2020</b>                  "<em>HM-ANN: Efficient Billion-Point Nearest Neighbor Search on Heterogeneous Memory</em>", Jie Ren, <b>Minjia Zhang</b>, Dong Li
    <li><b>SIGMOD 2020</b>                  "<em>Improving Approximate Nearest Neighbor Search through Learned Adaptive Early Termination</em>", Conglong Li, <b>Minjia Zhang</b>, Yuxiong He, David Anderson
</ul>

<h2>2019</h2>
<ul>
    <li><b>CIKM 2019</b>                         "<em>GRIP: Multi-Store Capacity-Optimized High-Performance Nearest Neighbor Search for Vector Search Engine</em>", <b>Minjia Zhang</b>, Yuxiong He
    <li><b>USENIX OpML 2019</b>  "<em>Accelerating Large Scale Deep Learning Inference through DeepCPU at Microsoft</em>",  <b>Minjia Zhang</b>, Samyam Rajbandari, Wenhan Wang, Elton Zheng, Olatunji Ruwase, Jeff Rasley, Jason Li, Junhua Wang, Yuxiong He</li>
</ul>

<h2>2018</h2>
<ul>
    <li><b>NeurIPS 2018</b>  “<em>Navigating with Graph Representations for Fast and Scalable Decoding of Neural Language Models</em>”, <b>Minjia Zhang</b>, Xiaodong Liu, Wenhan Wang, Jianfeng Gao, Yuxiong He
    <li><b>ICLR 2018</b>  “<em>Learning Intrinsic Sparse Structures within Long Short-Term Memory</em>”, Wei Wen, Yuxiong He, Samyam Rajbhandari, <b>Minjia Zhang</b>, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen, Hai Li
    <li><b>USENIX ATC 2018</b>  "<em>DeepCPU: Serving RNN-based Deep Learning Models 10x Faster</em>", <b>Minjia Zhang</b>*, Samyam Rajbhandari*, Wenhan Wang, Yuxiong He. *Equal contribution.</li>
</ul>

<h2>2017 and older</h2>
<ul>
  <li><b>TOPC 2017</b>                         "<em>Hybridizing and Relaxing Dependence Tracking for Efficient Parallel Runtime Support</em>", Man Cao, <b>Minjia Zhang</b>, Aritra Sengupta, Swarnendu Biswas, and Michael D. Bond, In ACM Transactions on Parallel Computing.
  <li><b>ISMM 2017</b>                       "<em>Avoiding Consistency Exceptions Under Strong Memory Consistency Models</em>", <b>Minjia Zhang</b>, Swarnendu Biswas, Michael D. Bond,  in the 2017 ACM SIGPLAN International Symposium on Memory Management.
  <li><b>CC 2017</b>                              "<em>Lightweight Data Race Detection for Production Runs</em>", Swarnendu Biswas, Man Cao, <b>Minjia Zhang</b>, Michael D. Bond, and Benjamin P. Wood, in the 26th International Conference on Compiler Construction.
  <li><b>PPoPP 2017</b>                     "<em>On the Problem of Consistency Exceptions in the Context of  Strong Memory Models</em>", <b>Minjia Zhang</b>, Swarnendu Biswas, Michael D. Bond, in the 22th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. 
  <li><b>CC 2016</b>                             "<em>Relaxed Dependence Tracking for Parallel Runtime Support</em>", <b>Minjia Zhang</b>, Swarnendu Biswas, Michael D. Bond, in the 25th International Conference on Compiler Construction. 
  <li><b>PPoPP 2016</b>                      "<em>Drinking from Both Glasses: Combining Pessimistic and Optimistic Tracking of Cross-Thread Dependences</em>", Man Cao, <b>Minjia Zhang</b>, Aritra Sengupta, and Michael Bond,  in the 21th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming.
  <li><b>OOPSLA 2015</b>                  "<em>Valor: Efficient, Software-Only Region Conflict Exceptions</em>"(Distinguished Artifact Award, Distinguished Paper Award), Swarnendu Biswas, <b>Minjia Zhang</b>, Michael D. Bond, and Brandon Lucia, in the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications.
  <li><b>PPoPP 2015</b>                    "<em>Low-Overhead Software Transactional Memory with Progress Guarantees and Strong Semantics</em>", <b>Minjia Zhang</b>, Jipeng Huang, Man Cao, and Michael D. Bond,  in the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming.
  <li><b>ASPLOS 2015</b>                  "<em>Hybrid Static-Dynamic Analysis for Statically Bounded Region Serializability</em>", Aritra Sengupta, Swarnendu Biswas, <b>Minjia Zhang</b>, Michael D. Bond, and Milind Kulkarni, in the 20th International Conference on Architectural Support for Programming Languages and Operating Systems.
  <li><b>SPLASH 2015 Companion</b>    "<em>SIRe: An Efficient Snapshot Isolation ­based Memory Model for Detecting and Tolerating Region Conflicts</em>", <b>Minjia Zhang</b>,  in 2015 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity.
  <li><b>WoDet 2014</b>                    "<em>Drinking from Both Glasses: Adaptively Combining Pessimistic and Optimistic Synchronization for Efficient Parallel Runtime Support</em>", Man Cao, <b>Minjia Zhang</b>, and Michael D. Bond,  in the 5th Workshop on Determinism and Correctness in Parallel Programming.
  <li><b>OOPSLA 2013</b>                  "<em>Octet: Capturing and Controlling Cross-Thread Dependences Efficiently</em>", Michael D. Bond, Milind Kulkarni, Man Cao, <b>Minjia Zhang</b>, Meisam Fathi Salmi, Swarnendu Biswas, Aritra Sengupta, and Jipeng Huang, in the 2013 ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications.
  <li><b>ICPP 2011</b>                            "<em>Memcached Design on High Performance RDMA Capable Interconnects</em>", J. Jose, H. Subramoni, M. Luo, M. Zhang, J. Huang, M. W. Rahman, N. S. Islam, X. Ouyang, S. Sur and D. K. Panda,  in the 40th International Conference on Parallel Processing.
  <li><b>ICPADS 2010</b>                   "<em>VirtCFT: A Transparent VM-level Fault-Tolerant System for Virtual Clusters</em>", <b>Minjia Zhang</b>,Hai Jin,Song Wu,Xuanhua Shi, in IEEE 16th International Conference on Parallel and Distributed Systems.
</ul>