
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html>
<head>
	<div class="navbar">
		<a href="./index.html"><font size="+1">Home</font></a>
		<a href="./publication.html"><font size="+1">Publications</font></a>
    <a href="./teaching.html"><font size="+1">Teaching</font></a>
    <a href="./students.html"><font size="+1">Students</font></a>
    <a href="./award.html"><font size="+1">Awards</font></a>
		<a href="./misc.html"><font size="+1">Misc</font></a>
	</div>
    <title>Minjia Zhang</title>
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <link rel="stylesheet" type="text/css" href="style.css"/>
</head>
<body style="color: rgba(0, 0, 0, 0.938);margin:0;padding:0">
<div id="wrapper">
  <div id="content-wrap">
    <div id="content">
      <div id="main">
        <tr>
        <p style="padding-right:30px;"> <img id="headshot" "float-right" align="right" alt="MinjiaZhang" src="Minjia-300x298.png" width="180" height="185"/></p>

            <p><font size="+3"><strong>&nbsp;Minjia Zhang</strong></font></p>
            <p><font size="+1"><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(he/him/his)</strong></font></p>
            <p></p>
            <p style="line-height:90%"><font size="3"><a href="https://cs.illinois.edu/" style="color:#9B0145;">&nbsp;&nbsp;The Grainger College of Engineering Computer Science</a></font></p>
            <p style="line-height:90%"><font size="3"><a href="https://illinois.edu/" style="color:#9B0145;">&nbsp;&nbsp;University of Illinois Urbana-Champaign</a></font></p>
            <p style="line-height:90%"><font size="3"><strong>&nbsp;&nbsp;Address:</strong> Thomas M. Siebel Center for Computer Science, 201 North Goodwin Avenue MC 258 Urbana, IL 61801</font></p>
            <p style="line-height:90%"><font size="3"><strong>&nbsp;&nbsp;Email:</strong> minjiaz|at|illinois|dot|edu</font></p>
            <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
            &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/citations?hl=en&user=98vX7S8AAAAJ"><i class="ai ai-google-scholar ai-2x" style="font-size:26px; color:#9B0145">&nbsp;</i></a>
	    <a href="https://www.semanticscholar.org/author/Minjia-Zhang/67016465"><i class="ai ai-semantic-scholar ai-2x" style="font-size:26px; color:#9B0145">&nbsp;</i></a>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
            <a href="https://www.linkedin.com/in/minjia-zhang-05857226/"><i class="fa fa-linkedin" style="font-size:26px; color:#9B0145">&nbsp;</i></a>
            <a href="https://dblp.org/pid/58/9033.html"><i class="fa fa-th-list" style="font-size:26px; color:#9B0145">&nbsp;</i></a>
        <!-- <p></p> -->

        </tr>
		<hr style="color:rgb(218, 218, 218);">

        <h1><a name="biography"; style="color:#40A6FF;">About Me</a></h1>
        <p> I am joning as an assistant professor (tenure-track) at the <a href="https://cs.illinois.edu/">Grainger College of Engineering Computer Science</a> of the <a href="https://illinois.edu/">University of Illinois Urbana-Champaign</a>. Prior to my appointment at UIUC, I had wonderful seven years at Microsoft Research Redmond and WebXT division as a Principal Researcher and technical lead. I have had all sorts of fun of developing highly efficient and cost-effective systems and algorithms, including but not limited to: enabling and accelerating large-scale deep learning training on parallel/distributed/heterogeneous hardware, building ultra-fast inference engine, different types of model compression, large-scale data management. My research works have been published in major venues, ranging from (i) system and high-performance computing conferences, such as ASPLOS, NSDI, USENIX ATC, PPoPP, SC, HPCA, (ii) top-tier machine learning conferences, including ICML, NeurIPS, ICLR, AAAI, etc, and (ii) data management conferences, such as SIGMOD, WSDM, CIKM. Several of my work has been applied to Microsoft systems and products, such as Bing, Ads, Azure SQL, Windows, etc., leading to significant latency improvement and cost reduction. 
        <p></p>
        I am also an early member of <a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>, an open-source deep learning optimization library that makes training and inference DL models easy, efficient, and effective. It has been widely adopted by both the industry and academia and have become a common backend for various popular DL frameworks such as HuggingFace, PyTorch Lightning, Fairscale, etc, and it has enabled training some of the largest language models in the world, such as Megatron-Turing 530B. I was also the <a href="https://docs.google.com/document/d/1ZThn1Tz_LA0Z2ayCfVyym9sCyedzgrUMAl0c1wMSgHc/edit">co-chair</a> of the engineering/scaling group of the <a href="https://bigscience.huggingface.co/">BigScience</a> project, helping training the BLOOM 176B model, which was once the world's largest open multilingual language model. Before DeepSpeed, I drive the <a href="https://www.microsoft.com/en-us/research/publication/deepcpu-serving-rnn-based-deep-learning-models-10x-faster/">DeepCPU</a> project at Microsoft, a DL inference optimization library that brought order-of-magnitude latency and cost reduction to production DL models.
        <p></p>
        Before joining Microsoft, I finished my Ph.D. from the Computer Science Department at <a href="https://cse.osu.edu/">Ohio State University</a> in May 2016, where I was a member of the <a href="https://mdbond.github.io/plass.html">PLaSS</a> group working on building efficient and scalable systems with strong semantics for parallel programs and advised by <a href="https://mdbond.github.io/">Prof. Michael D. Bond</a>. Along the way, I spent the summer of 2015, the summer/fall of 2016 at <a href="https://www.microsoft.com/en-us/research/">Microsoft Research Redmond</a>.
         
	  <p>&#128293; <a style="color:#d93b3b;"><b>Prospective students</b></a>: I am always looking for highly-motivated students who are intersted in systems or algorithms on machine learning, deep learning, and their applications in large-scale applications. Please feel free to reach out to me by sending me an email and apply to our <a href="https://cs.illinois.edu/admissions/graduate">graduate programs</a>.</p>   

    <dl style="background-color:rgba(230, 199, 243, 0.13)">
      <p></p>
      <a style="color:#6c3a3af2; font-size:18px"><b>&nbsp;&nbsp;Recent News</b></a>  
        <div class="myBox", style="overflow:scroll;height:200px">
          <ul>
            <li> [12/27/2023] Will be serving as a PC for USENIX ATC 2024.</i> 
            <li> [12/16/2023] I will be serving as a penalist at the 3rd Efficient Natural Language and Speech Processing (ENLSP) workshop at NeurIPS 2023, New Orleans. Thank you Yu, Yue, Medhi, and Soheila for the invitation!</i> 
            <li> [12/9/2023] Our paper on enabling efficient DNN training via data efficient optimizations has been accepted at AAAI 2024!</i> 
            <li> [12/7/2023] Our paper on enabling efficient DNN training on preemptible instances has been accepted at NSDI 2024! Congrats everyone!</a>
            <li> [12/27/2023] Will be serving as a reviewer for ICML 2024.</i>      
            <li> [10/30/2023] Will be serving as a reviewer for CVPR 2024.</i>        
            <li> [9/8/2023] Will be serving as a reviewer for MLSys 2024.</i> 
            <li> [8/24/2023] Will be serving as a reviewer for ICLR 2024.</i> 
            <li> [8/15/2023] Our paper on cost-effective on-device continual learning has been accepted at MobiCom 2023!</a>
            <li> [7/15/2023] Our paper on adversarial fine-tuning efficiency optimizations has been accepted at ECAI 2023!</a>
            <li> [1/21/2023] Our paper on compressed communication for large-scale training 0/1 Adam has been accepted at ICLR 2023!</a>
            <li> [11/7/2022] Our paper on fast and accurate vector search via intra-query parallelism has been accepted at PPoPP 2023!</a>
            <li> [9/20/2022] Our paper on large-scale GNN training on a single-node machine has been accepted at ASPLOS 2023!</li>
            <li> [9/14/2022] Three papers have been accepted at NeurIPS 2022! 2665 out of 10411 submissions are accepted.</li>
            <li> [7/8/2022] Our paper on large-scale DNN training on spot instances has been accepted at NSDI 2023! 50 out of 272 submissions are accepted.</li>
            <li> [6/13/2022] Our paper on large-scale inference for Transformer models has been accepted at SC 2022! 81 out of 320 submissions are accepted.</li>
            <li> [5/18/2023] Will be serving as a reviewer for ECAI 2023.</i> 
            <li> [05/5/2022] Our paper on advancing the next generation of AI via Mixture-of-Experts has been accepted at ICML 2022! 1117 out of 5630 submissions are accepted.</li>            
            <li> [3/21/2023] Will be serving as a PC for ASPLOS 2024.</i> 
            <li> [2/24/2022] Our paper on continual learning has been accepted at DAC 2022!</li>
            <li> [2/6/2023] Will be serving as a reviewer for ICCV 2023.</i> 
            <li> [12/1/2021] Our paper on adversarial data augmentation for knowledge distillation has been accepted at AAAI 2022! 1349 out of 9251 submissions are accepted.</li>
            <li> [10/11/2021] Our paper on graph sampling and pruning for nearest neighbor search has been accepted at WSDM 2022! 159 out of 786 submissions are accepted.</li>
            <li> [9/28/2021] Our paper on semi-structured sparsity for compressing Transformer networks has been accepted at NeurIPS 2021.</li>
          </ul>
        </div>
    </dl>

    <p></p>
    <h1><a name="publications"; style="color:#40A6FF;">Research Interests and Publications</h1>
    <a></a> 
    <h2>DNN Training at Scale and Speed: Breaking the Memory Wall and Beyond</h2>
    <ul>
          <li> <b>NSDI 2024</b>  "<em>Parcae: Proactive, Liveput-Optimized DNN Training on Preemptible Instances</em>", Jiangfei Duan, Ziang Song, Xupeng Miao, Xiaoli Xi, Dahua Lin, Harry Xu, <b>Minjia Zhang</b>, Zhihao Jia</li>
          <li> <b>AAAI 2024</b> "<em>DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing</em>", Conglong Li, Zhewei Yao, Xiaoxia Wu, <b>Minjia Zhang</b>, Connor Holmes, Cheng Li, Yuxiong He</li>
          <li> <b>ECAI 2023</b>  "<em>Revisiting the Efficiency-Accuracy Tradeoff in Adapting Transformer Models via Adversarial Fine-Tuning</em>", <b>Minjia Zhang</b>, Niranjan Uma Naresh, Yuxiong He</li>
          <li>  <b>ICLR  2023</b> "<em>Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam</em>", Yucheng Lu, Conglong Li, <b>Minjia Zhang</b>, Christopher De Sa, Yuxiong He</li>
          <li>  <b>ASPLOS 2023</b>  "<em>Betty: Enabling Large-Scale GNN Training with Batch-Level Graph Partitioning</em>", Shuangyan Yang, <b>Minjia Zhang</b>, Wenqian Dong, Dong Li</li>
          <li>  <b>NSDI 2023</b> "<em>Bamboo: Making Preemptible Instances Resilient for Affordable Training of Large DNNs</em>", John Thorpe, Pengzhan Zhao, Jonathan Eyolfson, Yifan Qiao, Zhihao Jia, <b>Minjia Zhang</b>, Ravi Netravali, Guoqing Harry Xu</li>
          <li>  <b>ICML 2022</b>  "<em>Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale</em>", Samyam Rajbhandari, Conglong Li, Zhewei Yao, <b>Minjia Zhang</b>, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He</li>
          <li>  <b>NeurIPS 2022</b> "<em>Curriculum learning: A regularization method for efficient and stable billion-scale gpt model pre-training</em>", Conglong Li, <b>Minjia Zhang</b>, Yuxiong He</li>
          <li>  <b>USENIX ATC 2021</b>  "<em>ZeRO-Offload: Democratizing Billion-Scale Model Training</em>", Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, <b>Minjia Zhang</b>, Dong Li, Yuxiong He</li>
          <li>  <b>HPCA 2021</b>  "<em>Efficient Tensor Migration and Allocation on Heterogeneous Memory Systems for Deep Learning</em>", Jie Ren, Jiaolin Luo, Kai Wu, <b>Minjia Zhang</b>, Hyeran Jeon, Dong Li</li>
          <li>  <b>NeurIPS 2020</b>  "<em>Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping</em>", <b>Minjia Zhang</b>, Yuxiong He</li>
    </ul>

    <h2>Production Needs Ultra-Fast Inference</h2>
    <ul>
        <li><b>SC 2022</b>  "<em>Enabling Efficient Inference of Transformer Models at Unprecedented Scale</em>", Reza Yazdani Aminabadi,  Samyam Rajbhandari, <b>Minjia Zhang</b>, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Yuxiong He</li> 
        <li><b>MLSys Workshop 2022</b>  "<em>A Survey of Multi-Tenant Deep Learning Inference on GPU</em>", Fuxun Yu, Di Wang, Longfei Shangguan, <b>Minjia Zhang</b>, Chenchen Liu, Xiang Chen, in the MLSYS'22 workshop on Cloud Intelligence/AIOps</li>
        <li><b>TECS 2022</b>  "<em>SHARP: An Adaptable, Energy-Efficient Accelerator for Recurrent Neural Network</em>", Reza Yazdani, Olatunji Ruwase, <b>Minjia Zhang</b>, Yuxiong He, Jose-Maria Arnau, Antonio Gonzalez, in the ACM Transactions on Embedded Computing Systems 2022</li>
        <li><b>ICLR 2021</b>                       "<em>DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation</em>", <b>Minjia Zhang</b>*, Menghao Li*, Chi Wang, Minqin Li. 
        <li><b>IPDPS 2021</b>                     "<em>DUET: Compiler-Aware Subgraph Scheduling for Tensor Programs on a Coupled CPU-GPU Architecture</em>", <b>Minjia Zhang</b>*, Zehua Hu*, Minqin Li. *Equal contribution.
        <li><b>NeurIPS 2020</b>                "<em>AdaTune: Adaptive Tensor Program Compilation Made Efficient</em>", Menghao Li*, <b>Minjia Zhang</b>*, Chi Wang, Minqin Li. *Equal contribution.
        <li><b>USENIX OpML 2019</b>  "<em>Accelerating Large Scale Deep Learning Inference through DeepCPU at Microsoft</em>",  <b>Minjia Zhang</b>, Samyam Rajbandari, Wenhan Wang, Elton Zheng, Olatunji Ruwase, Jeff Rasley, Jason Li, Junhua Wang, Yuxiong He</li>
        <li><b>USENIX ATC 2018</b>  "<em>DeepCPU: Serving RNN-based Deep Learning Models 10x Faster</em>", <b>Minjia Zhang</b>*, Samyam Rajbhandari*, Wenhan Wang, Yuxiong He. *Equal contribution.</li>
    </ul>

    <h2>Smaller, Faster, and Cheaper DNN via Model Compression</h2>
    <ul>
        <li><b>Preprint</b>   "<em>Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding</em>",  Connor Holmes, <b>Minjia Zhang</b>, Yuxiong He, Bo Wu
        <li><b>NeurIPS 2022</b>  "<em>Extreme Compression for Pre-trained Transformers Made Simple and Efficient</em>", Xiaoxia Wu, Zhewei Yao, <b>Minjia Zhang</b>, Conglong Li, Yuxiong He
        <li><b>NeurIPS 2022</b>   "<em>ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</em>", Zhewei Yao, Reza Yazdani Aminabadi, <b>Minjia Zhang</b>, Xiaoxia Wu, Conglong Li, Yuxiong He
        <li><b>AAAI 2022</b>   "<em>Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-Trained Transformers</em>", <b>Minjia Zhang</b>, Niranjan Uma Naresh, Yuxiong He
        <li><b>NeurIPS 2021</b>  "<em>NxMTransformer: Semi-Structured Sparsification for Natural Language Understanding via ADMM</em>", Connor Holmes, <b>Minjia Zhang</b>, Yuxiong He, Bo Wu
        <li><b>NeurIPS 2018</b>  “<em>Navigating with Graph Representations for Fast and Scalable Decoding of Neural Language Models</em>”, <b>Minjia Zhang</b>, Xiaodong Liu, Wenhan Wang, Jianfeng Gao, Yuxiong He
        <li><b>ICLR 2018</b>  “<em>Learning Intrinsic Sparse Structures within Long Short-Term Memory</em>”, Wei Wen, Yuxiong He, Samyam Rajbhandari, <b>Minjia Zhang</b>, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen, Hai Li
    </ul>

    <h2>AI4Science and LLM Applications</h2>
    <ul>
      <li><b>NeurIPS AI4Science Workshop</b>    "<em>DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies</em>",  Shuaiwen Leon Song, Bonnie Kruft, <b>Minjia Zhang</b>, Conglong Li, Shiyang Chen, Chengming Zhang, and all
      <li><b>Preprint</b>                     "<em>DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales</em>", Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, <b>Minjia Zhang</b>, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, Yuxiong He
      <li><b>Preprint</b>                   "<em>DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention</em>", Zhewei Yao, Xiaoxia Wu, Conglong Li, <b>Minjia Zhang</b>, Heyang Qi, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He
    </ul>

    <h2>Efficient Nearest Neighbor Methods and Their Applications in Large-scale Vector Search</h2>
    <ul>
      <li><b>PPoPP 2023</b>                      "<em>iQAN: Fast and Accurate Vector Search with Efficient Intra-Query Parallelism on Multi-Core Architectures</em>", Zhen Peng, <b>Minjia Zhang</b>, Kai Li, Ruoming Jin, Bin Ren
      <li><b>WSDM 2022</b>                    "<em>GraSP: Optimizing Graph-based Nearest Neighbor Search with Subgraph Sampling and Pruning</em>", <b>Minjia Zhang</b>, Wenhan Wang, Yuxiong He
      <li><b>NeurIPS 2020</b>                  "<em>HM-ANN: Efficient Billion-Point Nearest Neighbor Search on Heterogeneous Memory</em>", Jie Ren, <b>Minjia Zhang</b>, Dong Li
      <li><b>SIGMOD 2020</b>                  "<em>Improving Approximate Nearest Neighbor Search through Learned Adaptive Early Termination</em>", Conglong Li, <b>Minjia Zhang</b>, Yuxiong He, David Anderson
      <li><b>CIKM 2019</b>                         "<em>GRIP: Multi-Store Capacity-Optimized High-Performance Nearest Neighbor Search for Vector Search Engine</em>", <b>Minjia Zhang</b>, Yuxiong He
    </ul>

    <h2>Bringing AI to Edge: Running Powerful AI Programs on IoT and Low-Power Mobile Devices</h2>
    <ul>
      <li><b>MobiCom 2023</b>                 "<em>Cost-effective On-device Continual Learning over Memory Hierarchy with Miro</em>", Xinyue Ma, Suyeon Jeong, <b>Minjia Zhang</b>, Di Wang, Jonghyun Choi, Myeonjae Jeon
      <li><b>DAC 2022</b>                           "<em>CarM: Hierarchical Episodic Memory for Continual Learning</em>", Soobee Lee, Minindu Weerakoon, Jonghyun Choi, <b>Minjia Zhang</b>, Di Wang, Myeongjae Jeon
    </ul>

    <h2>Parallel Computing and Scalable Runtime</h2>
    <ul>
      <li><b>TOPC 2017</b>                         "<em>Hybridizing and Relaxing Dependence Tracking for Efficient Parallel Runtime Support</em>", Man Cao, <b>Minjia Zhang</b>, Aritra Sengupta, Swarnendu Biswas, and Michael D. Bond, In ACM Transactions on Parallel Computing.
      <li><b>ISMM 2017</b>                       "<em>Avoiding Consistency Exceptions Under Strong Memory Consistency Models</em>", <b>Minjia Zhang</b>, Swarnendu Biswas, Michael D. Bond,  in the 2017 ACM SIGPLAN International Symposium on Memory Management.
      <li><b>CC 2017</b>                              "<em>Lightweight Data Race Detection for Production Runs</em>", Swarnendu Biswas, Man Cao, <b>Minjia Zhang</b>, Michael D. Bond, and Benjamin P. Wood, in the 26th International Conference on Compiler Construction.
      <li><b>PPoPP 2017</b>                     "<em>On the Problem of Consistency Exceptions in the Context of  Strong Memory Models</em>", <b>Minjia Zhang</b>, Swarnendu Biswas, Michael D. Bond, in the 22th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. 
      <li><b>CC 2016</b>                             "<em>Relaxed Dependence Tracking for Parallel Runtime Support</em>", <b>Minjia Zhang</b>, Swarnendu Biswas, Michael D. Bond, in the 25th International Conference on Compiler Construction. 
      <li><b>PPoPP 2016</b>                      "<em>Drinking from Both Glasses: Combining Pessimistic and Optimistic Tracking of Cross-Thread Dependences</em>", Man Cao, <b>Minjia Zhang</b>, Aritra Sengupta, and Michael Bond,  in the 21th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming.
      <li><b>OOPSLA 2015</b>                  "<em>Valor: Efficient, Software-Only Region Conflict Exceptions</em>"(Distinguished Artifact Award, Distinguished Paper Award), Swarnendu Biswas, <b>Minjia Zhang</b>, Michael D. Bond, and Brandon Lucia, in the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications.
      <li><b>PPoPP 2015</b>                    "<em>Low-Overhead Software Transactional Memory with Progress Guarantees and Strong Semantics</em>", <b>Minjia Zhang</b>, Jipeng Huang, Man Cao, and Michael D. Bond,  in the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming.
      <li><b>ASPLOS 2015</b>                  "<em>Hybrid Static-Dynamic Analysis for Statically Bounded Region Serializability</em>", Aritra Sengupta, Swarnendu Biswas, <b>Minjia Zhang</b>, Michael D. Bond, and Milind Kulkarni, in the 20th International Conference on Architectural Support for Programming Languages and Operating Systems.
      <li><b>SPLASH 2015 Companion</b>    "<em>SIRe: An Efficient Snapshot Isolation ­based Memory Model for Detecting and Tolerating Region Conflicts</em>", <b>Minjia Zhang</b>,  in 2015 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity.
      <li><b>WoDet 2014</b>                    "<em>Drinking from Both Glasses: Adaptively Combining Pessimistic and Optimistic Synchronization for Efficient Parallel Runtime Support</em>", Man Cao, <b>Minjia Zhang</b>, and Michael D. Bond,  in the 5th Workshop on Determinism and Correctness in Parallel Programming.
      <li><b>OOPSLA 2013</b>                  "<em>Octet: Capturing and Controlling Cross-Thread Dependences Efficiently</em>", Michael D. Bond, Milind Kulkarni, Man Cao, <b>Minjia Zhang</b>, Meisam Fathi Salmi, Swarnendu Biswas, Aritra Sengupta, and Jipeng Huang, in the 2013 ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications.
      <li><b>ICPP 2011</b>                            "<em>Memcached Design on High Performance RDMA Capable Interconnects</em>", J. Jose, H. Subramoni, M. Luo, M. Zhang, J. Huang, M. W. Rahman, N. S. Islam, X. Ouyang, S. Sur and D. K. Panda,  in the 40th International Conference on Parallel Processing.
      <li><b>ICPADS 2010</b>                   "<em>VirtCFT: A Transparent VM-level Fault-Tolerant System for Virtual Clusters</em>", <b>Minjia Zhang</b>,Hai Jin,Song Wu,Xuanhua Shi, in IEEE 16th International Conference on Parallel and Distributed Systems.
    </ul>

    <p></p>
    <h1><a name="patents"; style="color:#40A6FF;">Patents</h1>
    <a></a> 
    <ul>
         <li> <b>Minjia Zhang</b>, Yuxiong He, "Multi-Layer Semantic Search", U.S. Patent, MS# 406007-US-NP, 2019</li>
         <li> <b>Minjia Zhang</b>, Xiaodong Liu, Wenhan Wang, Jianfeng Gao, Yuxiong He, “Graph Representations for Identifying a Next Word”, US 2019 / 0377792 A1</li>
         <li> <b>Minjia Zhang</b>, Samyam Rajbhandari, Wenhan Wang, Yuxiong He, “Deep Learning Model Scheduling”, US 2019 / 0311245 A1</li>
    </ul>

    <p></p>
     <h1><a name="talks"; style="color:#40A6FF;">Talks</h1>
     <a></a> 
     <ul>
          <li> Invited panel speaker at the Efficient Natural Language and Speech Processing (<a href="https://neurips2023-enlsp.github.io/">ENLSP-III</a>) workshop at NeurIPS 2023</li>  
          <li> Presented work on "Efficient System and Algorithm Design for Deep Learning Training and Inference", University of Illinois at Urbana-Champaign, Purdue University, University of Virginia, Univeresity of Minnesota, Indiana University Bloomington, Colorado School of Mines, Stevens Institute of Technology </li>
          <li> Presented work on "XTC: Extreme model compression made simple and efficient" at NeurIPS 2022 </li>
          <li> Invited talk on "Extreme Compression for Pre-trained Transformers Made Simple and Efficient" at Intel AI Group, July 28th 2022</li>
          <li> Invited talk by Zhihao Jia on "DeepSpeed: The library to accelerate training and inference of DNN at scale" at CMU, April 18th 2022</li>
          <li> Invited talk on "DeepSpeed: The library to accelerate training and inference of DNN at scale" at the Efficient Large-Scale AI Workshop as a part of MSR Project Green</li>
          <li> Invited talk by Myeongjae Jeon on "DeepSpeed: The library to accelerate training and inference of DNN at scale" at UNIST, April 13th 2022</li>
          <li> Invited lecture on "New algorithms for Approximate Nearest Neighbor Search Systems at Scale" at Kent State University, October 20, 2022</li>
          <li> Presented work on graph sampling and pruning for nearest neighbor search at WSDM 2022</li>
          <li> Invited talk on "DL Inference and Training Optimization Towards Speed and Scale" at Tsinghua AIR 2021</li>
          <li> Invited keynote speech on "DL Inference and Training Optimization Towards Speed and Scale" at EMDC 2021</li>
          <li> Presented work on DL inference through heterogeneous devices at IPDPS 2021</li>
          <li> Presented work on "DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation" at ICLR 2021</li>
          <li> Invited keynote speech on "DL Inference and Training Optimization Towards Speed and Scale" at EMDC 2021</li>
          <li> Presented work on "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping" at NeurIPS 2020</li>
          <li> Presented work on "AdaTune: Adaptive Tensor Program Compilation Made Efficient" at NeurIPS 2020</li>
          <li> Invited talk on "<a href="https://www.youtube.com/watch?v=WDo-k0syZi4&feature=youtu.be&list=PLTPQEx-31JXjA2ZmvYT5s0RqDXFXTSjyL&t=3547">TVM@Microsoft</a>" at the TVM and Deep Learning Compilation Conference 2019, Seattle, Washington, US</li>
          <li> Presented work on "GRIP: Multi-Store Capacity-Optimized High-Performance Nearest Neighbor Search for Vector Search Engine" at CIKM 2019, Beijing, China</li>
          <li> Presented work on "Accelerating Large Scale Deep Learning Inference through DeepCPU at Microsoft" at 2019 USENIX OpML, May 2019, Santa Clara, CA, USA</li>
          <li> Presented work on "DeepCPU: Serving RNN-based Deep Learning Models 10x Faster" at 2018 USENIX Annual Technical Conference, July 2018, Boston, MA, USA</li>
          <li> Invited talk on "DeepCPU: Deep Learning Serving Optimizations on CPUs" at the Deep Learning workshop at Microsoft TechFest 2018, March 2018, Redmond, WA, USA</li>
          <li> Invited talk on "DeepCPU: Deep Learning Serving Optimizations on CPUs" at Microsoft Research Talk Series, February 2018, Redmond, WA, USA</li>
          <li> Presented work on "DeepCPU: Deep Learning Serving Optimizations on CPUs" at Machine Learning, AI & Data Science Conference (MLADS) December 2017, Redmond, WA, USA</li>
          <li> Presented work on detecting and tolerating region conflicts to support region snapshot isolation at ACM Student Research Competition, OOPSLA 2015, Pittsburg, PA, USA</li>
          <li> Presented work on low-overhead and scalable software transactional memory with strong progress guarantees at the 20st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP 2015, San Francisco, CA, USA</li>
      </ul>

      <p></p>
        <h1><a name="activities"; style="color:#40A6FF;">Professional Service</h1>
        <a></a>  
        <ul>
            <li> <b>Organizer</b>: PLDI 2019 Publicitiy Co-Chair</li>
            <li> <b>Area/Session Chair</b>: ASPLOS 2019 Machine Learning Track Session Chair</li>
            <li> <b>Program Committee</b>: USENIX ATC 2024, ASPLOS 2023, ICDE 2023 Industry and Applications Track, MLSys 2023, IPDPS 2023, IPDPS 2021, IPDPS 2020, IPDPS 2019, IPDPS 2018, ASPLOS 2018 Shadow PC, PLDI 2017 Artifact Evaluation, SPLASH 2015 Artifact Evaluation, PLDI 2015 Artifact Evaluation</li>
            <li> <b>Conference Reviewer</b>: ICLR 2024, ICLR 2023, AAAI 2023, CVPR 2023, ICCV 2023, ECAI 2023, ICLR 2022, AAAI 2022, CVPR 2022, 
            USENIX ATC 2022, ICML 2022, ECCV 2022, NeurIPS 2022, ASPLOS 2021, AAAI 2021, ICLR 2021, CVPR 2021, ICCV 2021, ICML 2021, NeurIPS 2021, NeurIPS 2020, ICLR 2020, NeurIPS 2019, NeurIPS 2019 Reproducibility Challenge, PLDI 2019, ASPLOS 2019, Middleware 2018 subreviewer, ICAC 2018 subreviewer, IEEE Cloud 2018 subreviewer, HiPC 2017 subreviewer, ICAC 2017 subreviewer, WTTM 2015 subreviewer. </li>
            <li> <b>Journal Reviewer</b>: Transactions on Machine Learning Research (2022-2023), Journal of Systems and Software (2020), IEEE Transaction on Cloud Computing (2019-2020), ACM Transaction on Privacy and Security (2019), Journal of Computer Science (2017-2018), Concurrency and Computation: Practice and Experience (2016-2017)</li>   
        </ul>


		<hr style="color:rgb(218, 218, 218);">
		<div style="text-align: center; font-size: 18px;"><small>Last update: 12/2023</small>
		</div>
		<br>
      </div>
    </div>
  </div>
</div>

</body>
</html>
